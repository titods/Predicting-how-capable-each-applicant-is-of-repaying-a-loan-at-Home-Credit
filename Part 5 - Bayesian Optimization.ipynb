{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "multiple-external",
   "metadata": {},
   "source": [
    "# Load data and library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "growing-surveillance",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "pd.options.display.max_columns = None\n",
    "plt.rcParams.update(plt.rcParamsDefault)\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import sys\n",
    "import ast\n",
    "\n",
    "from sklearn.preprocessing import FunctionTransformer, MinMaxScaler, OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import cross_validate, KFold, train_test_split\n",
    "\n",
    "from lightgbm import LGBMClassifier\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "checked-sally",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to get the feature preprocessing pipeline\n",
    "def get_feature_pipeline(numerical, nominal, ordinal, algorithm):\n",
    "    preprocess_numerical = FunctionTransformer(lambda x: x[numerical], validate = False)\n",
    "    preprocess_nominal = FunctionTransformer(lambda x: x[nominal], validate = False)\n",
    "    preprocess_ordinal = FunctionTransformer(lambda x: x[ordinal], validate = False)\n",
    "    if algorithm == 'Logistic Regression':\n",
    "        pl_numerical = Pipeline([('selector_numerical', preprocess_numerical),\n",
    "                                 ('imputer', SimpleImputer(strategy = 'median')),\n",
    "                                 ('scaler', MinMaxScaler())])                    \n",
    "        pl_nominal = Pipeline([('selector_nominal', preprocess_nominal),\n",
    "                               ('imputer', SimpleImputer(strategy = 'most_frequent')),\n",
    "                               ('encoder', OneHotEncoder())])\n",
    "        pl_ordinal = Pipeline([('selector_ordinal', preprocess_ordinal),\n",
    "                               ('imputer', SimpleImputer(strategy = 'most_frequent')),\n",
    "                               ('encoder', OrdinalEncoder())])\n",
    "        feature_pipeline = FeatureUnion([('pipeline_numerical', pl_numerical),\n",
    "                                         ('pipeline_nominal', pl_nominal),\n",
    "                                         ('pipeline_ordinal', pl_ordinal)])\n",
    "        return feature_pipeline\n",
    "    \n",
    "    elif algorithm == 'GBM':\n",
    "        pl_numerical = Pipeline([('selector_numerical', preprocess_numerical)])\n",
    "        pl_nominal = Pipeline([('selector_nominal', preprocess_nominal),\n",
    "                               ('encoder', OneHotEncoder())])\n",
    "        pl_ordinal = Pipeline([('selector_ordinal', preprocess_ordinal),\n",
    "                               ('encoder', OrdinalEncoder())])\n",
    "        feature_pipeline = FeatureUnion([('pipeline_numerical', pl_numerical),\n",
    "                                         ('pipeline_nominal', pl_nominal),\n",
    "                                         ('pipeline_ordinal', pl_ordinal)])\n",
    "        return feature_pipeline\n",
    "    else:\n",
    "        print('algorithm argument is wrong. Try \"Logistic Regression\" or \"GBM\"!')\n",
    "        return None\n",
    "\n",
    "# helper function to evaluate the model performance\n",
    "def model_evaluation(model, train, test, name):\n",
    "    \"\"\" Parameter:\n",
    "            model - trained model\n",
    "            train - list of train feature and train target ---> [X_train, y_train]\n",
    "            test - test feature ---> X_test\n",
    "            name - trained model's name\n",
    "    \"\"\"\n",
    "    X_train = train[0]; y_train = train[1]\n",
    "    X_test = test\n",
    "    \n",
    "    # predict the model\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_proba_train = model.predict_proba(X_train)[:, 1]\n",
    "    y_pred_proba_test = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # training model performance\n",
    "    accuracy_train = metrics.accuracy_score(y_train, y_pred_train)\n",
    "    roc_auc_train = metrics.roc_auc_score(y_train, y_pred_proba_train)\n",
    "    roc_auc_cv = cross_validate(estimator = model, X = X_train, y = y_train, cv = 5, scoring = 'roc_auc', return_train_score = True)\n",
    "    dict_model_performance = {'Accuracy train':accuracy_train.round(3),\n",
    "                              'ROC AUC 5-CV train':'{} ± {}'.format(roc_auc_cv['train_score'].mean().round(3), roc_auc_cv['train_score'].std().round(3)),\n",
    "                              'ROC AUC 5-CV validate':'{} ± {}'.format(roc_auc_cv['test_score'].mean().round(3), roc_auc_cv['test_score'].std().round(3)), \n",
    "                              'ROC AUC train':roc_auc_train.round(3)}\n",
    "    print('\\n')\n",
    "    print ('======== model evaluation metrics \"{}\" ========'.format(name))\n",
    "    print('confusion matrix and classification report \"app_train\": \\n{0}\\n{1}'.format(metrics.confusion_matrix(y_train, y_pred_train), \n",
    "                                                                                      metrics.classification_report(y_train, y_pred_train, target_names = ['repaid', 'not repaid'])))\n",
    "    df = pd.DataFrame([dict_model_performance])\n",
    "    df.rename(index = {0:name}, inplace = True)\n",
    "    return df, y_pred_proba_test\n",
    "\n",
    "# get the feature importance from LGMB\n",
    "def feature_importance(model,plot = True, max_num_features = 15, figsize = (6, 4)):\n",
    "    numerical = list(model[0].transformer_list[0][1].steps[0][1].transform(app_train).columns)\n",
    "    ordinal = list(model[0].transformer_list[2][1].steps[1][1].feature_names_in_)\n",
    "    nominal = list(model[0].transformer_list[1][1].steps[1][1].get_feature_names_out())\n",
    "    all_columns = numerical + ordinal + nominal\n",
    "    df = pd.DataFrame({'Feature':all_columns, 'Number of Split':model[1].feature_importances_}).sort_values('Number of Split', ascending = True).set_index('Feature')\n",
    "    if plot == True:\n",
    "        fig, ax = plt.subplots(figsize = figsize)\n",
    "        df.tail(max_num_features).plot(kind = 'barh', ax = ax)\n",
    "        ax.set_title('Feature Importance LGBM')\n",
    "        plt.show()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "featured-burlington",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 28.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# load the final train and test dataset\n",
    "app_train = pd.read_csv('final train and test dataset/app_train_final.csv')\n",
    "app_test = pd.read_csv('final train and test dataset/app_test_final.csv')\n",
    "\n",
    "# load the column names\n",
    "column_names = pd.read_csv('final train and test dataset/column_names.csv')\n",
    "\n",
    "num_columns = ast.literal_eval(column_names[column_names['variable'] == 'num_columns']['list'].tolist()[0])\n",
    "cat_columns = ast.literal_eval(column_names[column_names['variable'] == 'cat_columns']['list'].tolist()[0])\n",
    "nom_columns = ast.literal_eval(column_names[column_names['variable'] == 'nom_columns']['list'].tolist()[0])\n",
    "ord_columns = ast.literal_eval(column_names[column_names['variable'] == 'ord_columns']['list'].tolist()[0])\n",
    "poly_columns = ast.literal_eval(column_names[column_names['variable'] == 'poly_columns']['list'].tolist()[0])\n",
    "creation_columns = ast.literal_eval(column_names[column_names['variable'] == 'creation_columns']['list'].tolist()[0])\n",
    "bureau_app_columns = ast.literal_eval(column_names[column_names['variable'] == 'bureau_app_columns']['list'].tolist()[0])\n",
    "previous_app_columns = ast.literal_eval(column_names[column_names['variable'] == 'previous_app_columns']['list'].tolist()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "certain-jurisdiction",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "app_train shape: (307511, 709)\n",
      "app_test shape: (48744, 708)\n"
     ]
    }
   ],
   "source": [
    "print('app_train shape:', app_train.shape)\n",
    "print('app_test shape:', app_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "charitable-gauge",
   "metadata": {},
   "outputs": [],
   "source": [
    "app_train['CODE_GENDER'] = pd.Categorical(app_train['CODE_GENDER'], ordered = True, categories = ['F', 'M'])\n",
    "app_test['CODE_GENDER'] = pd.Categorical(app_test['CODE_GENDER'], ordered = True, categories = ['F', 'M'])\n",
    "\n",
    "app_train['FLAG_OWN_CAR'] = pd.Categorical(app_train['FLAG_OWN_CAR'], ordered = True, categories = ['N', 'Y'])\n",
    "app_test['FLAG_OWN_CAR'] = pd.Categorical(app_test['FLAG_OWN_CAR'], ordered = True, categories = ['N', 'Y'])\n",
    "\n",
    "app_train['FLAG_OWN_REALTY'] = pd.Categorical(app_train['FLAG_OWN_REALTY'], ordered = True, categories = ['N', 'Y'])\n",
    "app_test['FLAG_OWN_REALTY'] = pd.Categorical(app_test['FLAG_OWN_REALTY'], ordered = True, categories = ['N', 'Y'])\n",
    "\n",
    "app_train['YEARS_BIRTH_SEGMENT'] = pd.Categorical(app_train['YEARS_BIRTH_SEGMENT'], ordered = True)\n",
    "app_test['YEARS_BIRTH_SEGMENT'] = pd.Categorical(app_test['YEARS_BIRTH_SEGMENT'], ordered = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liable-drill",
   "metadata": {},
   "source": [
    "# Hyperparameter tuning\n",
    "\n",
    "The best model we see from previous notebook `Part 2 - Modeling` is LGBM model using features from `num_columns`, `nom_columns`, `ord_columns`, `creation_columns`, `bureau_app_columns`, and `previous_app_columns`. Therefore we will try to tune LGBM's hyperparameter model using those features in order to get the better performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "greek-converter",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<307511x793 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 132771518 stored elements in Compressed Sparse Row format>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<48744x793 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 20640285 stored elements in Compressed Sparse Row format>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total features after preprocessing in Pipeline: 793\n",
      "Wall time: 39.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# preprocessing the features\n",
    "\n",
    "# set the feature pipeline\n",
    "LGBM_features_total = get_feature_pipeline(numerical = num_columns + creation_columns + bureau_app_columns + previous_app_columns, \n",
    "                                           nominal = nom_columns, \n",
    "                                           ordinal = ord_columns, \n",
    "                                           algorithm = 'GBM')\n",
    "# train data and test data\n",
    "X_train = LGBM_features_total.fit_transform(app_train)\n",
    "y_train = app_train[['TARGET']]\n",
    "X_test = LGBM_features_total.transform(app_test)\n",
    "display(X_train)\n",
    "display(X_test)\n",
    "\n",
    "# extract the features from preprocessing pipeline\n",
    "num_after = LGBM_features_total.transformer_list[0][1].steps[0][1].transform(app_train).columns.tolist()\n",
    "nom_after = LGBM_features_total.transformer_list[1][1].steps[1][1].get_feature_names_out().tolist()\n",
    "ord_after = LGBM_features_total.transformer_list[2][1].steps[1][1].feature_names_in_.tolist()\n",
    "all_columns_after = num_after + nom_after + ord_after\n",
    "print('Total features after preprocessing in Pipeline:', len(all_columns_after))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "urban-version",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(307511, 793) (307511, 1)\n",
      "(48744, 793)\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "# train set df\n",
    "train_features = pd.DataFrame(X_train.toarray(), columns = all_columns_after)\n",
    "train_features = train_features.rename(columns = lambda x:re.sub('[^A-Za-z0-9_]+', '', x))\n",
    "train_labels = y_train.copy()\n",
    "\n",
    "# test set df\n",
    "test_features = pd.DataFrame(X_test.toarray(), columns = all_columns_after)\n",
    "test_features = test_features.rename(columns = lambda x:re.sub('[^A-Za-z0-9_]+', '', x))\n",
    "\n",
    "print(train_features.shape, train_labels.shape)\n",
    "print(test_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "about-handling",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'verbose': -1}\n",
    "\n",
    "# training set\n",
    "train_set = lgb.Dataset(train_features, label = train_labels, params = params)\n",
    "test_set = lgb.Dataset(test_features, params = params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alternate-rabbit",
   "metadata": {},
   "source": [
    "## Bayesian optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hindu-patch",
   "metadata": {},
   "source": [
    "### Objective function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cardiovascular-fellow",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from hyperopt import STATUS_OK\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "def objective(hyperparameters):\n",
    "    \"\"\"Objective function for Gradient Boosting Machine Hyperparameter Optimization.\n",
    "       Writes a new line to `outfile` on every iteration\"\"\"\n",
    "    \n",
    "    # Keep track of evals\n",
    "    global ITERATION\n",
    "    ITERATION += 1\n",
    "    \n",
    "    # Using early stopping to find number of trees trained\n",
    "    if 'n_estimators' in hyperparameters:\n",
    "        del hyperparameters['n_estimators']\n",
    "    \n",
    "    # Make sure parameters that need to be integers are integers\n",
    "    for parameter_name in ['num_leaves', 'max_depth']:\n",
    "        hyperparameters[parameter_name] = int(hyperparameters[parameter_name])\n",
    "        \n",
    "    # Remove verbose in training process\n",
    "    hyperparameters['verbose'] = -1\n",
    "    start = timer()\n",
    "    \n",
    "    # Perform n_folds cross validation\n",
    "    cv_results = lgb.cv(hyperparameters, train_set, num_boost_round = 1000, nfold = N_FOLDS, \n",
    "                        early_stopping_rounds = 100, metrics = 'auc', seed = 420, verbose_eval = False)\n",
    "\n",
    "    run_time = timer() - start\n",
    "    \n",
    "    # Remove 'verbose' in hyperparameters dict\n",
    "    hyperparameters.pop('verbose')\n",
    "    \n",
    "    # Extract the best score\n",
    "    best_score = cv_results['auc-mean'][-1]\n",
    "    \n",
    "    # Loss must be minimized\n",
    "    loss = 1 - best_score\n",
    "    \n",
    "    # Boosting rounds that returned the highest cv score\n",
    "    n_estimators = len(cv_results['auc-mean'])\n",
    "    \n",
    "    # Add the number of estimators to the hyperparameters\n",
    "    hyperparameters['n_estimators'] = n_estimators\n",
    "\n",
    "    # Write to the csv file ('a' means append)\n",
    "    of_connection = open(OUT_FILE, 'a')\n",
    "    writer = csv.writer(of_connection)\n",
    "    writer.writerow([loss, hyperparameters, ITERATION, run_time, best_score])\n",
    "    of_connection.close()\n",
    "    print('ITERATION: {0} ---> time: {1}'.format(ITERATION, run_time))\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "initial-filling",
   "metadata": {},
   "source": [
    "### Domain search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "insured-california",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import hp\n",
    "from hyperopt.pyll.stochastic import sample\n",
    "\n",
    "# Define the search space\n",
    "space = {\n",
    "    'boosting_type': hp.choice('boosting_type', ['gbdt', 'dart', 'goss']),\n",
    "    'subsample': hp.uniform('subsample', 0.4, 1.0),\n",
    "    'num_leaves': hp.quniform('num_leaves', 20, 150, 1),\n",
    "    'max_depth': hp.quniform('max_depth', 1, 10, 1),\n",
    "    'learning_rate': hp.loguniform('learning_rate', np.log(0.05), np.log(0.5)),\n",
    "    'reg_alpha': hp.uniform('reg_alpha', 0.0, 1.0),\n",
    "    'reg_lambda': hp.uniform('reg_lambda', 0.0, 1.0),\n",
    "    'colsample_bytree': hp.uniform('colsample_by_tree', 0.5, 1.0),\n",
    "    'is_unbalance': hp.choice('is_unbalance', [True])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "surgical-delicious",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def check_dist(space, param):\n",
    "    value = []\n",
    "    for i in range(5000):\n",
    "        value.append(sample(space)[param])\n",
    "    plt.hist(value)\n",
    "    plt.title('Distribution of {}'.format(param))\n",
    "    plt.show()\n",
    "check_dist(space, 'learning_rate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bridal-ceremony",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'boosting_type': 'dart',\n",
       " 'colsample_bytree': 0.651324653372059,\n",
       " 'is_unbalance': True,\n",
       " 'learning_rate': 0.1963788472326696,\n",
       " 'max_depth': 6.0,\n",
       " 'num_leaves': 149.0,\n",
       " 'reg_alpha': 0.5393254723303981,\n",
       " 'reg_lambda': 0.3568693024594267,\n",
       " 'subsample': 0.4998174450741046}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'boosting_type': 'dart',\n",
       " 'colsample_bytree': 0.651324653372059,\n",
       " 'is_unbalance': True,\n",
       " 'learning_rate': 0.1963788472326696,\n",
       " 'max_depth': 6,\n",
       " 'num_leaves': 149,\n",
       " 'reg_alpha': 0.5393254723303981,\n",
       " 'reg_lambda': 0.3568693024594267,\n",
       " 'subsample': 0.4998174450741046}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# sample\n",
    "hyperparameters = sample(space)\n",
    "display(hyperparameters)\n",
    "\n",
    "if 'n_estimators' in hyperparameters:\n",
    "    del hyperparameters['n_estimators']\n",
    "\n",
    "for parameter_name in ['num_leaves', 'max_depth']:\n",
    "    hyperparameters[parameter_name] = int(hyperparameters[parameter_name])\n",
    "display(hyperparameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sorted-intensity",
   "metadata": {},
   "source": [
    "### Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "leading-indicator",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import fmin, Trials, tpe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "personalized-installation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a file and open a connection\n",
    "OUT_FILE = 'LGBM_total - Bayesian Optimization (tuned_LGBM_2).csv'\n",
    "of_connection = open(OUT_FILE, 'w')\n",
    "writer = csv.writer(of_connection)\n",
    "\n",
    "ITERATION = 0\n",
    "\n",
    "# Write column names\n",
    "headers = ['loss', 'hyperparameters', 'iteration', 'runtime', 'score']\n",
    "writer.writerow(headers)\n",
    "of_connection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "accompanied-grammar",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Record results\n",
    "trials = Trials()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fundamental-badge",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITERATION: 1 ---> time: 175.06616069999998                                                                             \n",
      "ITERATION: 2 ---> time: 274.17143200000004                                                                             \n",
      "ITERATION: 3 ---> time: 1506.0826756000001                                                                             \n",
      "ITERATION: 4 ---> time: 1802.7004607000001                                                                             \n",
      "ITERATION: 5 ---> time: 181.48142740000003                                                                             \n",
      "ITERATION: 6 ---> time: 156.40519749999976                                                                             \n",
      "ITERATION: 7 ---> time: 1535.6129174999996                                                                             \n",
      "ITERATION: 8 ---> time: 310.68315320000056                                                                             \n",
      "ITERATION: 9 ---> time: 136.39450120000038                                                                             \n",
      "ITERATION: 10 ---> time: 349.42419470000004                                                                            \n",
      "ITERATION: 11 ---> time: 140.52425179999955                                                                            \n",
      "ITERATION: 12 ---> time: 241.00668440000027                                                                            \n",
      "ITERATION: 13 ---> time: 1716.8250386999998                                                                            \n",
      "ITERATION: 14 ---> time: 236.54241739999998                                                                            \n",
      "ITERATION: 15 ---> time: 325.47141439999905                                                                            \n",
      "ITERATION: 16 ---> time: 1377.7907472999996                                                                            \n",
      "ITERATION: 17 ---> time: 2067.0419229                                                                                  \n",
      "ITERATION: 18 ---> time: 248.83368920000066                                                                            \n",
      "ITERATION: 19 ---> time: 251.43466620000072                                                                            \n",
      "ITERATION: 20 ---> time: 291.5968391999995                                                                             \n",
      "ITERATION: 21 ---> time: 259.1979742000003                                                                             \n",
      "ITERATION: 22 ---> time: 363.50184940000145                                                                            \n",
      "ITERATION: 23 ---> time: 1226.9796960000003                                                                            \n",
      "ITERATION: 24 ---> time: 1496.2559103999993                                                                            \n",
      "ITERATION: 25 ---> time: 1416.1122605000019                                                                            \n",
      "ITERATION: 26 ---> time: 1326.9796118000013                                                                            \n",
      "ITERATION: 27 ---> time: 1211.9883262000003                                                                            \n",
      "ITERATION: 28 ---> time: 933.8102591000024                                                                             \n",
      "ITERATION: 29 ---> time: 675.7986477000013                                                                             \n",
      "ITERATION: 30 ---> time: 1109.3791521999992                                                                            \n",
      "ITERATION: 31 ---> time: 1591.3716434999988                                                                            \n",
      "ITERATION: 32 ---> time: 1191.1881381999992                                                                            \n",
      "ITERATION: 33 ---> time: 1530.559345900001                                                                             \n",
      "ITERATION: 34 ---> time: 1320.5313490000008                                                                            \n",
      "ITERATION: 35 ---> time: 936.5542133999988                                                                             \n",
      "ITERATION: 36 ---> time: 993.4497702000008                                                                             \n",
      "ITERATION: 37 ---> time: 668.1239252999985                                                                             \n",
      "ITERATION: 38 ---> time: 188.07455819999814                                                                            \n",
      "ITERATION: 39 ---> time: 484.4464599999992                                                                             \n",
      "ITERATION: 40 ---> time: 1182.1987673999975                                                                            \n",
      "ITERATION: 41 ---> time: 130.41924190000282                                                                            \n",
      "ITERATION: 42 ---> time: 159.46287809999922                                                                            \n",
      "ITERATION: 43 ---> time: 1210.7243967000031                                                                            \n",
      "ITERATION: 44 ---> time: 529.1382604999962                                                                             \n",
      "ITERATION: 45 ---> time: 79.42789939999784                                                                             \n",
      "ITERATION: 46 ---> time: 211.04160459999548                                                                            \n",
      "ITERATION: 47 ---> time: 2661.740593499999                                                                             \n",
      "ITERATION: 48 ---> time: 1499.4212321999948                                                                            \n",
      "ITERATION: 49 ---> time: 377.2644704999984                                                                             \n",
      "ITERATION: 50 ---> time: 188.95675749999646                                                                            \n",
      "ITERATION: 51 ---> time: 1086.336770300004                                                                             \n",
      "ITERATION: 52 ---> time: 304.4670326999985                                                                             \n",
      "ITERATION: 53 ---> time: 941.525830300001                                                                              \n",
      "ITERATION: 54 ---> time: 117.64880599999742                                                                            \n",
      "ITERATION: 55 ---> time: 1394.9493447999994                                                                            \n",
      "ITERATION: 56 ---> time: 1687.1615938999967                                                                            \n",
      "ITERATION: 57 ---> time: 119.7161374999996                                                                             \n",
      "ITERATION: 58 ---> time: 1065.2085968999963                                                                            \n",
      "ITERATION: 59 ---> time: 364.55467789999966                                                                            \n",
      "ITERATION: 60 ---> time: 1965.2951517999973                                                                            \n",
      "ITERATION: 61 ---> time: 668.4777217999945                                                                             \n",
      "ITERATION: 62 ---> time: 208.15032919999794                                                                            \n",
      "ITERATION: 63 ---> time: 1191.3916014999995                                                                            \n",
      "ITERATION: 64 ---> time: 1156.6121612999996                                                                            \n",
      "ITERATION: 65 ---> time: 193.05285110000113                                                                            \n",
      "ITERATION: 66 ---> time: 1517.7638660000011                                                                            \n",
      "ITERATION: 67 ---> time: 1657.6240050999986                                                                            \n",
      "ITERATION: 68 ---> time: 1489.0750658999968                                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITERATION: 69 ---> time: 1825.5490734999985                                                                            \n",
      "ITERATION: 70 ---> time: 1802.0383072999975                                                                            \n",
      "ITERATION: 71 ---> time: 1600.726300800001                                                                             \n",
      "ITERATION: 72 ---> time: 1523.8783470999988                                                                            \n",
      "ITERATION: 73 ---> time: 957.4523647000024                                                                             \n",
      "ITERATION: 74 ---> time: 1219.7576905999958                                                                            \n",
      "ITERATION: 75 ---> time: 1411.8928038999875                                                                            \n",
      "ITERATION: 76 ---> time: 1184.7190255000023                                                                            \n",
      "ITERATION: 77 ---> time: 1099.1908345999982                                                                            \n",
      "ITERATION: 78 ---> time: 224.75750910000352                                                                            \n",
      "ITERATION: 79 ---> time: 1171.7302913000021                                                                            \n",
      "ITERATION: 80 ---> time: 1180.9199902999972                                                                            \n",
      "ITERATION: 81 ---> time: 177.98652390000643                                                                            \n",
      "ITERATION: 82 ---> time: 1492.414401800008                                                                             \n",
      "ITERATION: 83 ---> time: 944.8040577000065                                                                             \n",
      "ITERATION: 84 ---> time: 204.48966099999961                                                                            \n",
      "ITERATION: 85 ---> time: 1242.523729099994                                                                             \n",
      "ITERATION: 86 ---> time: 315.6517232000042                                                                             \n",
      "ITERATION: 87 ---> time: 1872.1144420000055                                                                            \n",
      "ITERATION: 88 ---> time: 965.606346200002                                                                              \n",
      "ITERATION: 89 ---> time: 131.6753170000011                                                                             \n",
      "ITERATION: 90 ---> time: 1553.2483457000053                                                                            \n",
      "ITERATION: 91 ---> time: 1176.1301983000012                                                                            \n",
      "ITERATION: 92 ---> time: 171.79111739998916                                                                            \n",
      "ITERATION: 93 ---> time: 1576.056504200009                                                                             \n",
      "ITERATION: 94 ---> time: 928.6418114999979                                                                             \n",
      "ITERATION: 95 ---> time: 145.00949100000435                                                                            \n",
      "ITERATION: 96 ---> time: 696.7055301999935                                                                             \n",
      "ITERATION: 97 ---> time: 482.2187457999971                                                                             \n",
      "ITERATION: 98 ---> time: 1236.8438898000022                                                                            \n",
      "ITERATION: 99 ---> time: 1420.2994943000085                                                                            \n",
      "ITERATION: 100 ---> time: 544.672748500001                                                                             \n",
      "100%|█████████████████████████████████████████| 100/100 [25:01:32<00:00, 900.92s/trial, best loss: 0.21668400110107278]\n",
      "Finished, best results\n",
      "[{'loss': 0.21668400110107278, 'status': 'ok'}]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'json' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-dc14812986a7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;31m# Save the trial results\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'bayesian_LGBM_total (tuned_LGBM_2) Trials.json'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'w'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m     \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrials_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'json' is not defined"
     ]
    }
   ],
   "source": [
    "MAX_EVALS = 100\n",
    "N_FOLDS = 5\n",
    "\n",
    "global ITERATION\n",
    "\n",
    "ITERATION = 0\n",
    "\n",
    "best = fmin(fn = objective, space = space, algo = tpe.suggest,\n",
    "            trials = trials, max_evals = MAX_EVALS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "different-bracket",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<hyperopt.base.Trials at 0x227bc2c0d00>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "# save the model to disk\n",
    "filename = 'LGBM_total - Trials Bayesian Optimization (tuned_LGBM_2).sav'\n",
    "pickle.dump(trials, open(filename, 'wb'))\n",
    "\n",
    "# load the model from disk\n",
    "loaded_model = pickle.load(open(filename, 'rb'))\n",
    "loaded_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stainless-queensland",
   "metadata": {},
   "source": [
    "### Fit the best params and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "dirty-alloy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>hyperparameters</th>\n",
       "      <th>iteration</th>\n",
       "      <th>runtime</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>0.216684</td>\n",
       "      <td>{'boosting_type': 'dart', 'colsample_bytree': ...</td>\n",
       "      <td>72</td>\n",
       "      <td>1523.878347</td>\n",
       "      <td>0.783316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0.216931</td>\n",
       "      <td>{'boosting_type': 'dart', 'colsample_bytree': ...</td>\n",
       "      <td>98</td>\n",
       "      <td>1236.843890</td>\n",
       "      <td>0.783069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>0.217050</td>\n",
       "      <td>{'boosting_type': 'dart', 'colsample_bytree': ...</td>\n",
       "      <td>75</td>\n",
       "      <td>1411.892804</td>\n",
       "      <td>0.782950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.217092</td>\n",
       "      <td>{'boosting_type': 'dart', 'colsample_bytree': ...</td>\n",
       "      <td>26</td>\n",
       "      <td>1326.979612</td>\n",
       "      <td>0.782908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.217158</td>\n",
       "      <td>{'boosting_type': 'dart', 'colsample_bytree': ...</td>\n",
       "      <td>25</td>\n",
       "      <td>1416.112261</td>\n",
       "      <td>0.782842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.232520</td>\n",
       "      <td>{'boosting_type': 'gbdt', 'colsample_bytree': ...</td>\n",
       "      <td>9</td>\n",
       "      <td>136.394501</td>\n",
       "      <td>0.767480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.233585</td>\n",
       "      <td>{'boosting_type': 'gbdt', 'colsample_bytree': ...</td>\n",
       "      <td>18</td>\n",
       "      <td>248.833689</td>\n",
       "      <td>0.766415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.234917</td>\n",
       "      <td>{'boosting_type': 'dart', 'colsample_bytree': ...</td>\n",
       "      <td>35</td>\n",
       "      <td>936.554213</td>\n",
       "      <td>0.765083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.238108</td>\n",
       "      <td>{'boosting_type': 'dart', 'colsample_bytree': ...</td>\n",
       "      <td>16</td>\n",
       "      <td>1377.790747</td>\n",
       "      <td>0.761892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>0.271940</td>\n",
       "      <td>{'boosting_type': 'dart', 'colsample_bytree': ...</td>\n",
       "      <td>87</td>\n",
       "      <td>1872.114442</td>\n",
       "      <td>0.728060</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        loss                                    hyperparameters  iteration  \\\n",
       "71  0.216684  {'boosting_type': 'dart', 'colsample_bytree': ...         72   \n",
       "97  0.216931  {'boosting_type': 'dart', 'colsample_bytree': ...         98   \n",
       "74  0.217050  {'boosting_type': 'dart', 'colsample_bytree': ...         75   \n",
       "25  0.217092  {'boosting_type': 'dart', 'colsample_bytree': ...         26   \n",
       "24  0.217158  {'boosting_type': 'dart', 'colsample_bytree': ...         25   \n",
       "..       ...                                                ...        ...   \n",
       "8   0.232520  {'boosting_type': 'gbdt', 'colsample_bytree': ...          9   \n",
       "17  0.233585  {'boosting_type': 'gbdt', 'colsample_bytree': ...         18   \n",
       "34  0.234917  {'boosting_type': 'dart', 'colsample_bytree': ...         35   \n",
       "15  0.238108  {'boosting_type': 'dart', 'colsample_bytree': ...         16   \n",
       "86  0.271940  {'boosting_type': 'dart', 'colsample_bytree': ...         87   \n",
       "\n",
       "        runtime     score  \n",
       "71  1523.878347  0.783316  \n",
       "97  1236.843890  0.783069  \n",
       "74  1411.892804  0.782950  \n",
       "25  1326.979612  0.782908  \n",
       "24  1416.112261  0.782842  \n",
       "..          ...       ...  \n",
       "8    136.394501  0.767480  \n",
       "17   248.833689  0.766415  \n",
       "34   936.554213  0.765083  \n",
       "15  1377.790747  0.761892  \n",
       "86  1872.114442  0.728060  \n",
       "\n",
       "[100 rows x 5 columns]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bayesian_results = pd.read_csv('LGBM_total - Bayesian Optimization (tuned_LGBM_2).csv').sort_values(by = 'loss')\n",
    "bayesian_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "suspected-spice",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'boosting_type': 'dart',\n",
       " 'colsample_bytree': 0.5590968433055882,\n",
       " 'is_unbalance': True,\n",
       " 'learning_rate': 0.06924087417014092,\n",
       " 'max_depth': 6,\n",
       " 'num_leaves': 48,\n",
       " 'reg_alpha': 0.9624493183295199,\n",
       " 'reg_lambda': 0.373874171328887,\n",
       " 'subsample': 0.6090034292579527,\n",
       " 'n_estimators': 1000}"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# best hyperparameters\n",
    "hyperparameters = ast.literal_eval(bayesian_results.head(1).hyperparameters.tolist()[0])\n",
    "hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "hairy-headset",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline]  (step 1 of 2) Processing preprocessing_features, total= 1.5min\n",
      "[Pipeline] .............. (step 2 of 2) Processing LGBM, total= 6.6min\n",
      "done...\n",
      "Wall time: 8min 16s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# create model for LGBM: tuned_LGBM_2\n",
    "LGBM_features_total = get_feature_pipeline(numerical = num_columns + creation_columns + bureau_app_columns + previous_app_columns, \n",
    "                                           nominal = nom_columns, \n",
    "                                           ordinal = ord_columns, \n",
    "                                           algorithm = 'GBM')\n",
    "\n",
    "tuned_LGBM_2 = LGBMClassifier(**hyperparameters, random_state = 42, \n",
    "                              class_weight = 'balanced', objective = 'binary', n_jobs = -1)\n",
    "\n",
    "tuned_LGBM_2 = Pipeline([('preprocessing_features', LGBM_features_total),\n",
    "                         ('LGBM', tuned_LGBM_2)], verbose = True)\n",
    "\n",
    "tuned_LGBM_2.fit(app_train.drop('TARGET', axis = 1),\n",
    "                 app_train[['TARGET']])\n",
    "print('done...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "ambient-privacy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LGBMClassifier(boosting_type='dart', class_weight='balanced',\n",
       "               colsample_bytree=0.5590968433055882, is_unbalance=True,\n",
       "               learning_rate=0.06924087417014092, max_depth=6,\n",
       "               n_estimators=1000, num_leaves=48, objective='binary',\n",
       "               random_state=42, reg_alpha=0.9624493183295199,\n",
       "               reg_lambda=0.373874171328887, subsample=0.6090034292579527)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "# save the model to disk\n",
    "filename = 'LGBM_total - Best Params (tuned_LGBM_2).sav'\n",
    "pickle.dump(tuned_LGBM_2[1], open(filename, 'wb'))\n",
    "\n",
    "# load the model from disk\n",
    "loaded_model = pickle.load(open(filename, 'rb'))\n",
    "loaded_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "given-pavilion",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "======== model evaluation metrics \"tuned_LGBM_2\" ========\n",
      "confusion matrix and classification report \"app_train\": \n",
      "[[ 44739 237947]\n",
      " [     0  24825]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      repaid       1.00      0.16      0.27    282686\n",
      "  not repaid       0.09      1.00      0.17     24825\n",
      "\n",
      "    accuracy                           0.23    307511\n",
      "   macro avg       0.55      0.58      0.22    307511\n",
      "weighted avg       0.93      0.23      0.27    307511\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy train</th>\n",
       "      <th>ROC AUC 5-CV train</th>\n",
       "      <th>ROC AUC 5-CV validate</th>\n",
       "      <th>ROC AUC train</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>tuned_LGBM_2</th>\n",
       "      <td>0.226</td>\n",
       "      <td>0.898 ± 0.001</td>\n",
       "      <td>0.765 ± 0.002</td>\n",
       "      <td>0.884</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Accuracy train ROC AUC 5-CV train ROC AUC 5-CV validate  \\\n",
       "tuned_LGBM_2           0.226      0.898 ± 0.001         0.765 ± 0.002   \n",
       "\n",
       "              ROC AUC train  \n",
       "tuned_LGBM_2          0.884  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 30min 12s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# evaluate tuned_LGBM_2\n",
    "df_tuned_LGBM_2, y_pred_proba_test = model_evaluation(model = tuned_LGBM_2[1], \n",
    "                                                      train = [X_train, y_train], \n",
    "                                                      test = X_test, \n",
    "                                                      name = 'tuned_LGBM_2')\n",
    "display(df_tuned_LGBM_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "boxed-contrast",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SK_ID_CURR</th>\n",
       "      <th>TARGET</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100001</td>\n",
       "      <td>0.486146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100005</td>\n",
       "      <td>0.938105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100013</td>\n",
       "      <td>0.348974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100028</td>\n",
       "      <td>0.722819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100038</td>\n",
       "      <td>0.939451</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   SK_ID_CURR    TARGET\n",
       "0      100001  0.486146\n",
       "1      100005  0.938105\n",
       "2      100013  0.348974\n",
       "3      100028  0.722819\n",
       "4      100038  0.939451"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create submission dataframe and save the submission to a csv file\n",
    "df_submit = app_test[['SK_ID_CURR']].copy()\n",
    "df_submit['TARGET'] = y_pred_proba_test\n",
    "df_submit.to_csv('submission/tuned_LGBM_2.csv', index = False)\n",
    "df_submit.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "sweet-washington",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy train</th>\n",
       "      <th>ROC AUC 5-CV train</th>\n",
       "      <th>ROC AUC 5-CV validate</th>\n",
       "      <th>ROC AUC train</th>\n",
       "      <th>ROC AUC test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>tuned_LGBM_2</th>\n",
       "      <td>0.226</td>\n",
       "      <td>0.898 ± 0.001</td>\n",
       "      <td>0.765 ± 0.002</td>\n",
       "      <td>0.884</td>\n",
       "      <td>0.755</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Accuracy train ROC AUC 5-CV train ROC AUC 5-CV validate  \\\n",
       "tuned_LGBM_2           0.226      0.898 ± 0.001         0.765 ± 0.002   \n",
       "\n",
       "              ROC AUC train  ROC AUC test  \n",
       "tuned_LGBM_2          0.884         0.755  "
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tuned_LGBM_2['ROC AUC test'] = round(0.75505, 3)\n",
    "df_tuned_LGBM_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sixth-adolescent",
   "metadata": {},
   "source": [
    "- The `ROC AUC test` score is obtained after submit the submission file to [Kaggle](https://www.kaggle.com/competitions/home-credit-default-risk/overview) that is `0.75505`.\n",
    "- The model overfit the training set."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "320px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
